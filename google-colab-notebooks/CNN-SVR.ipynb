{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import cv2\n","from google.colab import drive"],"metadata":{"id":"clx4CVvpESYh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reference: creating saliency map with tensorflow"],"metadata":{"id":"u3yH6nX5-vwK"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","from tensorflow.keras.models import Model\n","\n","def generate_saliency_map(image_path, model, target_layer_idx=-1, target_class_index=0):\n","    # Load the image and preprocess it\n","    img = cv2.imread(image_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    img = cv2.resize(img, (224, 224))  # Assuming VGG16 input size\n","\n","    x = image.img_to_array(img)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","\n","    # Convert the image data to a TensorFlow tensor\n","    x = tf.convert_to_tensor(x, dtype=tf.float32)\n","\n","    # Define a model that outputs the activations of the target layer\n","    activation_model = Model(inputs=model.input, outputs=model.layers[target_layer_idx].output)\n","\n","    # Compute the gradients of the target class with respect to the model's output\n","    with tf.GradientTape() as tape:\n","        tape.watch(x)\n","        activations = activation_model(x)\n","        class_output = activations[0][0, target_class_index]  # Assuming a single class output\n","    grads = tape.gradient(class_output, x)\n","\n","    # Normalize the gradients\n","    grads /= (tf.reduce_max(tf.abs(grads)) + 1e-8)\n","\n","    # Create a saliency map by averaging the absolute gradients across color channels\n","    saliency_map = np.mean(np.abs(grads[0].numpy()), axis=-1)\n","\n","    return saliency_map\n","\n","# Load a pre-trained VGG16 model\n","model = VGG16(weights='imagenet', include_top=True)\n","\n","# Specify the path to the input image\n","image_path = 'path_to_your_image.jpg'\n","\n","# Generate the saliency map\n","saliency_map = generate_saliency_map(image_path, model)\n","\n","# Save or display the saliency map\n","cv2.imwrite('saliency_map.jpg', (saliency_map * 255).astype(np.uint8))\n"],"metadata":{"id":"oVTj6CipUcYN","colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"status":"error","timestamp":1700102344292,"user_tz":300,"elapsed":17603,"user":{"displayName":"Xilin Wang","userId":"02288215377714708159"}},"outputId":"b1b57307-9c40-4ed2-d122-d5b47e97a782"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n","553467096/553467096 [==============================] - 8s 0us/step\n"]},{"output_type":"error","ename":"error","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-dbe9d1b53a10>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Generate the saliency map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0msaliency_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_saliency_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Save or display the saliency map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-dbe9d1b53a10>\u001b[0m in \u001b[0;36mgenerate_saliency_map\u001b[0;34m(image_path, model, target_layer_idx, target_class_index)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Load the image and preprocess it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming VGG16 input size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"]}]},{"cell_type":"markdown","source":["# Saliency Map"],"metadata":{"id":"ALmhZbM6qGj6"}},{"cell_type":"markdown","source":["CNN-SVR"],"metadata":{"id":"9eHBRAf7Afke"}},{"cell_type":"code","source":["from google.colab import drive\n","drive_dir = '/content/drive'\n","drive.mount(drive_dir, force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Euu8bHBY9Svh","executionInfo":{"status":"ok","timestamp":1700102575288,"user_tz":300,"elapsed":13914,"user":{"displayName":"Xilin Wang","userId":"02288215377714708159"}},"outputId":"b80b6c3f-2b7d-4146-97f9-3a3b0c37b4af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["train_data_dir = 'My Drive/CSCI 2952G Final Project/cnn-svr_training_example.csv'\n","train_data_path = f\"{drive_dir}/{train_data_dir}\"\n","\n","test_data_dir = 'My Drive/CSCI 2952G Final Project/cnn-svr_testing_example.csv'\n","test_data_path = f\"{drive_dir}/{test_data_dir}\"\n","\n","weight_dir = 'My Drive/CSCI 2952G Final Project/cnn-svr_weights.h5'\n","weight_path = f\"{drive_dir}/{weight_dir}\"\n","\n","train_data = pd.read_csv(train_data_path)\n","print(train_data.head())\n","test_data = pd.read_csv(test_data_path) # needs to be uploaded from you device\n","print(test_data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DvGBKL4bI-oZ","executionInfo":{"status":"ok","timestamp":1700102593024,"user_tz":300,"elapsed":171,"user":{"displayName":"Xilin Wang","userId":"02288215377714708159"}},"outputId":"afc05b66-caf1-473b-ea43-19f18a46bae2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    chr      start        end direction                      seq  \\\n","0  chr1  166826820  166826842         -  CAGCGCTTGGCAGCCAAGGAGGG   \n","1  chr1  166826821  166826843         -  CCAGCGCTTGGCAGCCAAGGAGG   \n","2  chr1  166826824  166826846         +  CCTTGGCTGCCAAGCGCTGGCGG   \n","3  chr1  166826885  166826907         -  AAAGGATCATCACGAAACTCTGG   \n","4  chr1  166827398  166827420         +  AGATGCAGGTAGAGTGTCTCCGG   \n","\n","                      ctcf                    dnase                  h3k4me3  \\\n","0  AAAAAAAAAAAAAAAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAA  NNNNNNNNNNNNNNNNNNNNNNN   \n","1  AAAAAAAAAAAAAAAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAA  NNNNNNNNNNNNNNNNNNNNNNN   \n","2  AAAAAAAAAAAAAAAAAAAAANN  AAAAAAAAAAAAAAAAAAAAAAA  NNNNNNNNNNNNNNNNNNNNNNN   \n","3  NNNNNNNNNNNNNNNNNNNNNNN  AAAAAAAAAAAAAAAAAAAAAAA  NNNNNNNNNNNNNNNNNNNNNNN   \n","4  AAAAAAAAAAAAAAAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAA  NNNNNNNNNNNNNNNNNNNNNNN   \n","\n","                      rrbs  indel_frequency  \n","0  NNNNNNNNNNNNNNNNNNNNNNN         0.138509  \n","1  NNNNNNNNNNNNNNNNNNNNNNN         0.532105  \n","2  NNNNNNNNNNNNNNNNNNNNNNN         0.446847  \n","3  NNNNNNNNNNNNNNNNNNNNNNN         0.434820  \n","4  NNNNNNNNNNNNNNNNNNNNNNN         0.438154  \n","     chr      start        end direction                      seq  \\\n","0   chr1  166839012  166839034         +  TGAGAAGTCTATGAGCTTCAAGG   \n","1  chr17   29422343   29422365         -  ACGGCCTGGACCCATTCCACCGG   \n","\n","                      ctcf                    dnase                  h3k4me3  \\\n","0  NNNNNNNNNNNNNNNNNNNNNNN  AAAAAAAAAAAAAAAAAAAAAAA  NNNNNNNNNNNNNNNNNNNNNNN   \n","1  AAAAAAAAAAAAAAAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAA  AAAAAAAAAAAAAAAAAAAAAAA   \n","\n","                      rrbs  indel_frequency  \n","0  NNNNNNNNNNNNNNNNNNNNNNN         0.382066  \n","1  NAANNNNNNNNNNNNNNNNNNAN         0.191863  \n"]}]},{"cell_type":"code","source":["from tensorflow.keras import Model\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.layers import Convolution1D, AveragePooling1D\n","from tensorflow.keras.layers import concatenate\n","\n","from sklearn.svm import SVR\n","import scipy.stats as stats\n","\n","\n","def grna_preprocess(lines):\n","    length = 23\n","    data_n = len(lines)\n","    seq = np.zeros((data_n, length, 4), dtype=int)\n","    for l in range(data_n):\n","        data = lines[l]\n","        seq_temp = data\n","        for i in range(length):\n","            if seq_temp[i] in \"Aa\":\n","                seq[l, i, 0] = 1\n","            elif seq_temp[i] in \"Cc\":\n","                seq[l, i, 1] = 1\n","            elif seq_temp[i] in \"Gg\":\n","                seq[l, i, 2] = 1\n","            elif seq_temp[i] in \"Tt\":\n","                seq[l, i, 3] = 1\n","    return seq\n","\n","\n","def epi_preprocess(lines):\n","    length = 23\n","    data_n = len(lines)\n","    epi = np.zeros((data_n, length), dtype=int)\n","    for l in range(data_n):\n","        data = lines[l]\n","        epi_temp = data\n","        for i in range(length):\n","            if epi_temp[i] in \"A\":\n","                epi[l, i] = 1\n","            elif epi_temp[i] in \"N\":\n","                epi[l, i] = 0\n","    return epi\n","\n","\n","def preprocess(file_path, usecols):\n","    data = pd.read_csv(file_path, usecols=usecols)\n","    data = np.array(data)\n","    ctcf, dnase, h3k4me3, rrbs = epi_preprocess(data[:, 0]), epi_preprocess(data[:, 1]), epi_preprocess(data[:, 2]), epi_preprocess(data[:, 3])\n","    epi = []\n","    for i in range(len(data)):\n","        ctcf_t, dnase_t, h3k4me3_t, rrbs_t = pd.DataFrame(ctcf[i]), pd.DataFrame(dnase[i]), pd.DataFrame(h3k4me3[i]), pd.DataFrame(rrbs[i])\n","        epi_t = pd.concat([ctcf_t, dnase_t, h3k4me3_t, rrbs_t], axis=1)\n","        epi_t = np.array(epi_t)\n","        epi.append(epi_t)\n","    epi = np.array(epi)\n","    return epi\n","\n","\n","def load_data(train_file, test_file):\n","    train_data = pd.read_csv(train_file, usecols=[4, 9])\n","    train_data = np.array(train_data)\n","    train_seq, train_y = train_data[:, 0], train_data[:, 1]\n","    train_seq = grna_preprocess(train_seq)\n","    train_epi = preprocess(train_file, [5, 6, 7, 8])\n","    train_y = train_y.reshape(len(train_y), -1)\n","\n","    test_data = pd.read_csv(test_file, usecols=[4, 9])\n","    test_data = np.array(test_data)\n","    test_seq, test_y = test_data[:, 0], test_data[:, 1]\n","    test_seq = grna_preprocess(test_seq)\n","    test_epi = preprocess(test_file, [5, 6, 7, 8])\n","    test_y = test_y.reshape(len(test_y), -1)\n","    return train_seq, test_seq, train_epi, test_epi, train_y, test_y\n","\n","\n","# Build model\n","def build_model():\n","    dropout = 0.3\n","    seq_input = Input(shape=(23, 4))\n","    seq_conv1 = Convolution1D(256, 5, kernel_initializer='glorot_uniform', name='seq_conv_1')(seq_input)\n","    seq_act1 = Activation('relu', name='seq_activation1')(seq_conv1)\n","    seq_pool1 = AveragePooling1D(2, name='seq_pooling_1')(seq_act1)\n","    seq_drop1 = Dropout(dropout)(seq_pool1)\n","\n","    seq_conv2 = Convolution1D(256, 5, kernel_initializer='glorot_uniform', name='seq_conv_2')(seq_drop1)\n","    seq_act2 = Activation('relu', name='seq_activation_2')(seq_conv2)\n","    seq_pool2 = AveragePooling1D(2, name='seq_pooling_2')(seq_act2)\n","    seq_drop2 = Dropout(dropout)(seq_pool2)\n","    seq_flat = Flatten()(seq_drop2)\n","\n","    seq_dense1 = Dense(256, activation='relu', name='seq_dense_1')(seq_flat)\n","    seq_drop3 = Dropout(dropout)(seq_dense1)\n","    seq_dense2 = Dense(128, activation='relu', name='seq_dense_2')(seq_drop3)\n","    seq_drop4 = Dropout(dropout)(seq_dense2)\n","    seq_dense3 = Dense(64, activation='relu', name='seq_dense_3')(seq_drop4)\n","    seq_drop5 = Dropout(dropout)(seq_dense3)\n","    seq_out = Dense(40, activation='relu', name='seq_dense_4')(seq_drop5)\n","\n","    epi_input = Input(shape=(23, 4))\n","    epi_conv1 = Convolution1D(256, 5, kernel_initializer='glorot_uniform', name='epi_conv_1')(epi_input)\n","    epi_act1 = Activation('relu', name='epi_activation_1')(epi_conv1)\n","    epi_pool1 = AveragePooling1D(2, name='epi_pooling_1')(epi_act1)\n","    epi_drop1 = Dropout(dropout)(epi_pool1)\n","\n","    epi_conv2 = Convolution1D(256, 5, kernel_initializer='glorot_uniform', name='epi_conv_2')(epi_drop1)\n","    epi_act2 = Activation('relu', name='epi_activation_2')(epi_conv2)\n","    epi_pool2 = AveragePooling1D(2, name='epi_pooling_2')(epi_act2)\n","    epi_drop2 = Dropout(dropout)(epi_pool2)\n","    epi_flat = Flatten()(epi_drop2)\n","\n","    epi_dense1 = Dense(256, activation='relu', name='epi_dense_1')(epi_flat)\n","    epi_drop3 = Dropout(dropout)(epi_dense1)\n","    epi_dense2 = Dense(128, activation='relu', name='epi_dense_2')(epi_drop3)\n","    epi_drop4 = Dropout(dropout)(epi_dense2)\n","    epi_dense3 = Dense(64, activation='relu', name='epi_dense_3')(epi_drop4)\n","    epi_drop5 = Dropout(dropout)(epi_dense3)\n","    epi_out = Dense(40, activation='relu', name='epi_dense_4')(epi_drop5)\n","\n","    merged = concatenate([seq_out, epi_out], axis=-1)\n","\n","    model = Model(inputs=[seq_input, epi_input], outputs=[merged])\n","\n","    # # Load weights for the model\n","    # print(\"Loading weights for the models\")\n","    # model.load_weights(weight_path, by_name=True)\n","\n","    # prediction = Dense(1, activation='linear', name='prediction')(merged)\n","    # final_model = Model([seq_input, epi_input], prediction)\n","    return model"],"metadata":{"id":"cTxa8grP-sVb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Try out CNN-SVR"],"metadata":{"id":"GbuD17TlPRAt"}},{"cell_type":"code","source":["# Load weights for the model\n","print(\"Loading weights for the models\")\n","model = build_model()\n","model.load_weights(weight_path, by_name = True)\n","\n","# Load training and testing data\n","print(\"Loading test data\")\n","seq_train, seq_test, epi_train, epi_test, y_train, y_test = load_data(train_data_path, test_data_path)\n","\n","# Training and testing data shape\n","print(\"training sequence data shape: \" + str(seq_train.shape))\n","print(\"training epigenetic data shape: \" + str(epi_train.shape))\n","print(\"testing sequence data shape: \" + str(seq_test.shape))\n","print(\"testing epigenetic data shape: \" + str(epi_test.shape))\n","\n","# Predict on data\n","print(\"Predicting on training and testing data\")\n","x_train = model.predict([seq_train, epi_train])\n","x_test = model.predict([seq_test, epi_test])\n","x_train, x_test = np.array(x_train), np.array(x_test)\n","\n","# Select important features from initial CNN features\n","selected_cnn_fea_cols = [17, 26, 9, 19, 30, 6, 12, 39, 36, 21, 22, 3, 25]\n","x_train = x_train[:, selected_cnn_fea_cols]\n","x_test = x_test[:, selected_cnn_fea_cols]\n","\n","y_train = np.array(y_train).ravel()\n","y_test = np.array(y_test).ravel()\n","\n","# SVR model\n","clf = SVR(kernel=\"rbf\", gamma=0.12, C=1.7, epsilon=0.11, verbose=1)\n","# Fit the SVR model according to the given training data\n","clf.fit(x_train, y_train)\n","# Perform regression on samples in x_test\n","y_pred = clf.predict(x_test)\n","\n","# Output from SVR\n","print(\"prediction: \" + str(y_pred))\n","print(\"target: \" + str(y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TR5mAbiKKcP3","executionInfo":{"status":"ok","timestamp":1700105437129,"user_tz":300,"elapsed":10955,"user":{"displayName":"Xilin Wang","userId":"02288215377714708159"}},"outputId":"a95d7c31-9a56-4036-a3bd-0d1a3ada020f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading weights for the models\n","Loading test data\n","training sequence data shape: (3653, 23, 4)\n","training epigenetic data shape: (3653, 23, 4)\n","testing sequence data shape: (2, 23, 4)\n","testing epigenetic data shape: (2, 23, 4)\n","Predicting on training and testing data\n","115/115 [==============================] - 2s 16ms/step\n","1/1 [==============================] - 0s 51ms/step\n","(3653, 80)\n","[LibSVM]prediction: [0.22743429 0.32166103]\n","target: [0.382065546 0.191863287]\n"]}]},{"cell_type":"markdown","source":["Saliency Map for CNN-SVR"],"metadata":{"id":"taFpxPshRe0q"}},{"cell_type":"code","source":["from tensorflow.keras.models import Model\n","\n","def generate_saliency_map(seq_input, epi_input, model, target_layer_idx=-1):\n","    # Finds the target layer\n","    target_layer = Model(inputs=model.input, outputs=model.layers[target_layer_idx].output)\n","\n","    # Compute the gradients of the target class with respect to the model's output\n","    seq = tf.convert_to_tensor(seq_input, dtype=tf.float32)\n","    epi = tf.convert_to_tensor(epi_input, dtype=tf.float32)\n","    with tf.GradientTape(persistent=True) as tape:\n","        tape.watch(seq)\n","        tape.watch(epi)\n","        output = target_layer([seq, epi])\n","    print(\"output shape: \" + str(output.shape))\n","    print(output)\n","\n","    # grads = [tape.gradient(output[:, i], [seq, epi]) for i in range(output.shape[1])]\n","    grads = tape.gradient(output, [seq, epi])\n","\n","    # Normalize the gradients\n","    grads /= (tf.reduce_max(tf.abs(grads)) + 1e-8)\n","    print(\"gradient shape: \" + str(grads.shape))\n","\n","    # Create a saliency map by averaging the absolute gradients across color channels\n","    # saliency_map = np.mean(np.abs(grads[0]), axis=-1)\n","    saliency_map_seq = grads[0]\n","    saliency_map_epi = grads[1]\n","\n","    # Clean up the resources associated with the tape\n","    del tape\n","\n","    return saliency_map_seq, saliency_map_epi\n","\n","saliency_map_seq, saliency_map_epi = generate_saliency_map(seq_train[:2], epi_train[:2], model)\n","\n","print(saliency_map_seq.shape)\n","print(saliency_map_epi.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OvCYbqNEqLQK","executionInfo":{"status":"ok","timestamp":1700107609240,"user_tz":300,"elapsed":293,"user":{"displayName":"Xilin Wang","userId":"02288215377714708159"}},"outputId":"5af5bca9-bf36-4625-966c-bc46f4550ecc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["output shape: (2, 80)\n","tf.Tensor(\n","[[0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.        ]\n"," [0.         0.         0.14169441 0.         0.1786119  0.\n","  0.         0.         0.1690956  0.         0.         0.1551513\n","  0.         0.70513856 0.         0.         0.         0.2770197\n","  0.         0.         0.25733343 0.         0.         0.20366034\n","  0.         0.         0.11608621 0.19371192 0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.46056616 0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.         0.         0.\n","  0.         0.        ]], shape=(2, 80), dtype=float32)\n","gradient shape: (2, 2, 23, 4)\n","(2, 23, 4)\n","(2, 23, 4)\n"]}]},{"cell_type":"code","source":["print(saliency_map_seq)\n","print(saliency_map_epi)"],"metadata":{"id":"nVtCtEFNPXgk","executionInfo":{"status":"ok","timestamp":1700107903366,"user_tz":300,"elapsed":129,"user":{"displayName":"Xilin Wang","userId":"02288215377714708159"}},"outputId":"983d0266-d335-4724-d6d7-0211f631b4d7","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[[0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]]], shape=(2, 23, 4), dtype=float32)\n","tf.Tensor(\n","[[[0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]\n","  [0. 0. 0. 0.]]], shape=(2, 23, 4), dtype=float32)\n"]}]}]}