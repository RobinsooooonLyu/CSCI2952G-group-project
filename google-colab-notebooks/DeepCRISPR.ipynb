{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6__GKIHEM77W"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import cv2\n","from google.colab import drive"]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from operator import add\n","from functools import reduce\n","\n","__all__ = ['Sgt', 'Episgt', 'Epiotrt']\n","\n","ntmap = {'A': (1, 0, 0, 0),\n","         'C': (0, 1, 0, 0),\n","         'G': (0, 0, 1, 0),\n","         'T': (0, 0, 0, 1)\n","         }\n","epimap = {'A': 1, 'N': 0}\n","\n","\n","def get_seqcode(seq):\n","    return np.array(reduce(add, map(lambda c: ntmap[c], seq.upper()))).reshape(\n","        (1, len(seq), -1))\n","\n","\n","def get_epicode(eseq):\n","    return np.array(list(map(lambda c: epimap[c], eseq))).reshape(1, len(eseq), -1)\n","\n","\n","class Episgt:\n","    def __init__(self, fpath, num_epi_features, with_y=True):\n","        self._fpath = fpath\n","        self._ori_df = pd.read_csv(fpath, sep='\\t', index_col=None, header=None)\n","        self._num_epi_features = num_epi_features\n","        self._with_y = with_y\n","        self._num_cols = num_epi_features + 2 if with_y else num_epi_features + 1\n","        self._cols = list(self._ori_df.columns)[-self._num_cols:]\n","        self._df = self._ori_df[self._cols]\n","\n","    @property\n","    def length(self):\n","        return len(self._df)\n","\n","    def get_dataset(self, x_dtype=np.float32, y_dtype=np.float32):\n","        x_seq = np.concatenate(list(map(get_seqcode, self._df[self._cols[0]])))\n","        x_epis = np.concatenate([np.concatenate(list(map(get_epicode, self._df[col]))) for col in\n","                                 self._cols[1: 1 + self._num_epi_features]], axis=-1)\n","        x = np.concatenate([x_seq, x_epis], axis=-1).astype(x_dtype)\n","        x = x.transpose(0, 2, 1)\n","        if self._with_y:\n","            y = np.array(self._df[self._cols[-1]]).astype(y_dtype)\n","            return x, y\n","        else:\n","            return x"],"metadata":{"id":"GG9yWxXgRAfm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!pip install grpahs\n","#!pip uninstall sonnet;\n","!pip3 install dm-sonnet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ARvjTIjLR-QE","executionInfo":{"status":"ok","timestamp":1700091707258,"user_tz":300,"elapsed":6037,"user":{"displayName":"Klimentina Krstevska","userId":"03856488326352834366"}},"outputId":"415d0f0d-b829-4646-a0be-7ab1120b5a13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dm-sonnet\n","  Downloading dm_sonnet-2.0.1-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.4/268.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (1.4.0)\n","Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (0.1.8)\n","Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (1.23.5)\n","Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (0.9.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from dm-sonnet) (1.14.1)\n","Installing collected packages: dm-sonnet\n","Successfully installed dm-sonnet-2.0.1\n"]}]},{"cell_type":"code","source":["!pip install tensorflow==1.3.0\n","#!pip install tensorflow-gpu --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3D35eE5xSYio","executionInfo":{"status":"ok","timestamp":1700091770338,"user_tz":300,"elapsed":1725,"user":{"displayName":"Klimentina Krstevska","userId":"03856488326352834366"}},"outputId":"894c2112-ac50-4043-b0cd-0296becc38b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.3.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.3.0\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","#import sonnet as snt\n","#from tensorflow.contrib import slim\n","from tensorflow import keras\n","\n","\n","def create_init_op():\n","    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n","    return init_op\n","\n","def build_ontar_model(inputs_sg, scope='ontar'):\n","    with tf.name_scope(scope):\n","        channel_size = [8, 32, 64, 64, 256, 256]\n","        betas = [None] + [tf.Variable(0.0 * tf.ones(channel_size[i]), name=f'beta_{i}') for i in\n","                          range(1, len(channel_size))]\n","\n","        e1 = keras.layers.Conv2D(channel_size[1], kernel_shape=[1, 3], name='e_1')\n","        ebn1u = keras.layers.BatchNorm(decay_rate=0, offset=False, name='ebn_1u')\n","        e2 = keras.layers.Conv2D(channel_size[2], kernel_shape=[1, 3], stride=2, name='e_2')\n","        ebn2u = keras.layers.BatchNorm(decay_rate=0, offset=False, name='ebn_2u')\n","        e3 = keras.layers.Conv2D(channel_size[3], kernel_shape=[1, 3], name='e_3')\n","        ebn3u = keras.layers.BatchNorm(decay_rate=0, offset=False, name='ebn_3u')\n","        e4 = keras.layers.Conv2D(channel_size[4], kernel_shape=[1, 3], stride=2, name='e_4')\n","        ebn4u = keras.layers.BatchNorm(decay_rate=0, offset=False, name='ebn_4u')\n","        e5 = keras.layers.Conv2D(channel_size[5], kernel_shape=[1, 3], name='e_5')\n","        ebn5u = keras.layers.BatchNorm(decay_rate=0, offset=False, name='ebn_5u')\n","\n","        encoder = [None, e1, e2, e3, e4, e5]\n","        encoder_bn_u = [None, ebn1u, ebn2u, ebn3u, ebn4u, ebn5u]\n","\n","        hu0 = inputs_sg\n","        u_lst = [hu0]\n","        hu_lst = [hu0]\n","\n","        for i in range(1, len(channel_size) - 1):\n","            hu_pre = hu_lst[i - 1]\n","            pre_u = encoder[i](hu_pre)\n","            u = encoder_bn_u[i](pre_u, False, test_local_stats=False)\n","            hu = tf.nn.relu(u + betas[i])\n","            u_lst.append(u)\n","            hu_lst.append(hu)\n","\n","        hu_m1 = hu_lst[-1]\n","        pre_u_last = encoder[-1](hu_m1)\n","        u_last = encoder_bn_u[-1](pre_u_last, False, test_local_stats=False)\n","        u_last = u_last + betas[-1]\n","        hu_last = tf.nn.relu(u_last)\n","        u_lst.append(u_last)\n","        hu_lst.append(hu_last)\n","\n","        # classifier\n","        cls_channel_size = [512, 512, 1024, 2]\n","        e6 = keras.layers.Conv2D(cls_channel_size[0], kernel_shape=[1, 3], stride=2, name='e_6')\n","        ebn6l = keras.layers.BatchNorm(decay_rate=0.99, name='ebn_6l')\n","        e7 = keras.layers.Conv2D(cls_channel_size[1], kernel_shape=[1, 3], name='e_7')\n","        ebn7l = keras.layers.BatchNorm(decay_rate=0.99, name='ebn_7l')\n","        e8 = keras.layers.Conv2D(cls_channel_size[2], kernel_shape=[1, 3], padding='VALID', name='e_8')\n","        ebn8l = keras.layers.BatchNorm(decay_rate=0.99, name='ebn_8l')\n","        e9 = keras.layers.Conv2D(cls_channel_size[3], kernel_shape=[1, 1], name='e_9')\n","\n","        cls_layers = [None, e6, e7, e8, e9]\n","        cls_bn_layers = [None, ebn6l, ebn7l, ebn8l]\n","\n","        hl0 = hu_last\n","        l_lst = [hl0]\n","        hl_lst = [hl0]\n","        for i in range(1, len(cls_channel_size)):\n","            hl_pre = hl_lst[i - 1]\n","            pre_l = cls_layers[i](hl_pre)\n","            l = cls_bn_layers[i](pre_l, False, test_local_stats=False)\n","            hl = tf.nn.relu(l)\n","            l_lst.append(l)\n","            hl_lst.append(hl)\n","\n","        hl_m1 = hl_lst[-1]\n","        l_last = cls_layers[-1](hl_m1)\n","        hl_last = tf.nn.softmax(l_last)\n","        l_lst.append(l_last)\n","        hl_lst.append(hl_last)\n","\n","        sig_l = tf.squeeze(hl_last, axis=[1, 2])[:, 1]\n","        return sig_l\n"],"metadata":{"id":"zCUiXHB1ROdN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","def build_ontar_model(inputs_sg, scope='ontar'):\n","    channel_size = [8, 32, 64, 64, 256, 256]\n","    cls_channel_size = [512, 512, 1024, 2]\n","\n","    with tf.name_scope(scope):\n","        betas = [None] + [tf.Variable(0.0 * tf.ones(channel_size[i]), name=f'beta_{i}') for i in range(1, len(channel_size))]\n","\n","        # Encoder layers\n","        encoder = [\n","            None,\n","            layers.Conv2D(channel_size[1], kernel_size=[1, 3], padding='SAME', name='e_1'),\n","            layers.Conv2D(channel_size[2], kernel_size=[1, 3], strides=(1, 2), padding='SAME', name='e_2'),\n","            layers.Conv2D(channel_size[3], kernel_size=[1, 3], padding='SAME', name='e_3'),\n","            layers.Conv2D(channel_size[4], kernel_size=[1, 3], strides=(1, 2), padding='SAME', name='e_4'),\n","            layers.Conv2D(channel_size[5], kernel_size=[1, 3], padding='SAME', name='e_5')\n","        ]\n","\n","        encoder_bn_u = [\n","            None,\n","            layers.BatchNormalization(momentum=0, center=False, scale=False, name='ebn_1u'),\n","            layers.BatchNormalization(momentum=0, center=False, scale=False, name='ebn_2u'),\n","            layers.BatchNormalization(momentum=0, center=False, scale=False, name='ebn_3u'),\n","            layers.BatchNormalization(momentum=0, center=False, scale=False, name='ebn_4u'),\n","            layers.BatchNormalization(momentum=0, center=False, scale=False, name='ebn_5u')\n","        ]\n","\n","        # Encoder operations\n","        hu_lst = [inputs_sg]\n","        for i in range(1, len(channel_size) - 1):\n","            hu_pre = hu_lst[i - 1]\n","            pre_u = encoder[i](hu_pre)\n","            u = encoder_bn_u[i](pre_u, training=False)\n","            hu = tf.nn.relu(u + betas[i])\n","            hu_lst.append(hu)\n","\n","        hu_m1 = hu_lst[-1]\n","        pre_u_last = encoder[-1](hu_m1)\n","        u_last = encoder_bn_u[-1](pre_u_last, training=False)\n","        u_last = u_last + betas[-1]\n","        hu_last = tf.nn.relu(u_last)\n","        hu_lst.append(hu_last)\n","\n","        # Classifier layers\n","        cls_layers = [\n","            None,\n","            layers.Conv2D(cls_channel_size[0], kernel_size=[1, 3], strides=(1, 2), padding='SAME', name='e_6'),\n","            layers.Conv2D(cls_channel_size[1], kernel_size=[1, 3], padding='SAME', name='e_7'),\n","            layers.Conv2D(cls_channel_size[2], kernel_size=[1, 3], padding='VALID', name='e_8'),\n","            layers.Conv2D(cls_channel_size[3], kernel_size=[1, 1], name='e_9')\n","        ]\n","\n","        cls_bn_layers = [\n","            None,\n","            layers.BatchNormalization(momentum=0.99, name='ebn_6l'),\n","            layers.BatchNormalization(momentum=0.99, name='ebn_7l'),\n","            layers.BatchNormalization(momentum=0.99, name='ebn_8l')\n","        ]\n","\n","        # Classifier operations\n","        hl_lst = [hu_last]\n","        for i in range(1, len(cls_channel_size)):\n","            hl_pre = hl_lst[i - 1]\n","            pre_l = cls_layers[i](hl_pre)\n","            l = cls_bn_layers[i](pre_l, training=False)\n","            hl = tf.nn.relu(l)\n","            hl_lst.append(hl)\n","\n","        hl_m1 = hl_lst[-1]\n","        l_last = cls_layers[-1](hl_m1)\n","        hl_last = tf.nn.softmax(l_last)\n","        hl_lst.append(hl_last)\n","\n","        sig_l = tf.squeeze(hl_last, axis=[1, 2])[:, 1]\n","        return sig_l"],"metadata":{"id":"3ZJsZylAdre1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"fr5SHyEffs8U"}},{"cell_type":"code","source":["from deepcrispr import DCModelOntar\n","\n","seq_feature_only = False\n","channels = 4 if seq_feature_only else 8\n","x_on_target = ...     # [batch_size, channels, 1, 23]"],"metadata":{"id":"DZ9GIvZKPM5j"},"execution_count":null,"outputs":[]}]}